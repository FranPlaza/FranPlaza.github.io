{
  "hash": "29ea739ecb68c7b2326b67a8ce7f59b1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelos de Deep Learning\"\nsubtitle: \"Descripción del curso\"\n# author:\n#   - name: Francisco Plaza Vega\n#     email: francisco.plaza.v@usach.cl\n#     affiliation: Departamento de Matemática y Ciencia de la Computación, Universidad de Santiago de Chile\n# date: 11/02/23\n# date-format: \"DD/MM/YY\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-depth: 2\n    toc-expand: 2\n    html-math-method: katex\n    theme: simplex\nlang: es\nhighlight-style: github\ntoc: false\ntoc-title: \"Contenidos\"\ntoc-expand: false\ntoc-depth: 2\nnumber-sections: true\nnumber-depth: 2\nbibliography: refs.bib\n#title-block-banner: false\n#image: cover.png\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Objetivos del curso \n\n::: {.callout-tip appearance=\"simple\" icon=false}\n- Comprender y aplicar los principios fundamentales de Deep Learning para modelar y resolver problemas complejos en diversos dominios.\n- Diseñar, entrenar y validar modelos de Deep Learning, utilizando conjuntos de datos reales y simulados, para tareas de clasificación, regresión y otras aplicaciones avanzadas.\n- Evaluar críticamente la efectividad y eficiencia de diferentes arquitecturas de redes neuronales, incluidas las redes convolucionales y recurrentes, en la solución de problemas específicos.\n-\tDesarrollar habilidades prácticas en el uso de herramientas y librerías de software de vanguardia para el Deep Learning, tales como TensorFlow, PyTorch, y/o Keras, permitiendo la simulación y experimentación con modelos complejos.\n-\tImplementar técnicas de optimización y ajuste fino para mejorar el rendimiento de los modelos de aprendizaje profundo, así como aplicar métodos para verificar y validar la precisión de los modelos desarrollados.\n:::\n\n# Programa del curso\n\n::: {.callout-tip collapse=\"true\" icon=false}\n## Unidad 1: Fundamentos del Deep Learning\n\n    1.1. Introducción al Deep Learning y su evolución histórica.\n    1.2. Principios básicos de las redes neuronales: perceptrones, perceptrones multicapa (MLP), funciones de activación, y propagación hacia adelante.\n    1.3. Entrenamiento de redes neuronales profundas: backpropagation, descenso del gradiente y ajuste de hiperparámetros. Uso de la librería Keras para facilitar la implementación de modelos.\n    1.4. Evaluación de modelos: conjuntos de entrenamiento, validación y prueba; sobreajuste y técnicas de regularización.\n    1.5. Introducción a TensorFlow, PyTorch y Keras como interfaz de alto nivel para la construcción y el entrenamiento de modelos de deep learning.\n\n[Slides](Deep_Learning_01.html){target=\"_blank\"} </br>\n[Tutorial básico de Python](Deep_Learning_01_python.html){target=\"_blank\"}\n:::\n\n::: {.callout-tip collapse=\"true\" icon=false}\n## Unidad 2: Perceptrones Multicapa (MLP) y Deep Learning\n\n    2.1. Estructura y características de los MLP: capas ocultas, nodos y profundidad.\n    2.2. Aplicaciones prácticas de los MLP en clasificación y regresión.\n    2.3. Comparación entre MLP y otros modelos de Deep Learning en términos de capacidad, complejidad y tipos de problemas a resolver.\n    2.4. Implementación de MLP para tareas de clasificación y regresión.\n    2.5. Estrategias de optimización y ajuste fino específicas para MLP.\n\n[Slides](Deep_Learning_02.html){target=\"_blank\"} </br>\n[Guía de Keras](Deep_Learning_02_guide_01.html){target=\"_blank\"} </br>\n[Ejemplo 1: Librería `neuralnet` en R](Deep_Learning_02_examples_01.html){target=\"_blank\"} </br>\n[Ejemplo 2: Librería `keras` con datos de MNIST](Deep_Learning_02_examples_02.html){target=\"_blank\"}</br>\n[Ejemplo 3: Librería `keras` para regresión con datos de automóviles](Deep_Learning_02_examples_03.html){target=\"_blank\"}\n\n:::\n\n::: {.callout-tip collapse=\"true\" icon=false}\n## Unidad 3: Redes Neuronales Convolucionales (CNNs)\n\n    3.1.\tFundamentos de las CNNs: operación de convolución, pooling y normalización.\n    3.2.\tArquitecturas de CNNs populares: LeNet, AlexNet, VGG, ResNet e Inception.\n    3.3.\tIntroducción al Transfer Learning en CNNs: cómo reutilizar modelos preentrenados para nuevas tareas.\n    3.4.\tAplicaciones de las CNNs: reconocimiento de imágenes, detección de objetos y segmentación semántica.\n    3.5.\tPrácticas con conjuntos de datos reales utilizando librerías de Deep Learning.\n    \n[Slides](Deep_Learning_03_CNN.html){target=\"_blank\"} </br>\n[Ejemplo 1: Librería `Keras` con datos `fashion-MNIST`](Deep_Learning_03_examples_01.html){target=\"_blank\"} </br>\n[Ejemplo 2: Gatos y perros](Deep_Learning_03_examples_02.html){target=\"_blank\"} </br>\n[Ejemplo 2b: Gatos y perros, la venganza](Deep_Learning_03_examples_02b.html){target=\"_blank\"}\n:::\n\n::: {.callout-tip collapse=\"true\" icon=false}\n## Unidad 4: Redes Neuronales Recurrentes (RNNs) y LSTM\n\n    4.1.\tPrincipios de las RNNs: procesamiento de secuencias y dependencias temporales.\n    4.2.\tProblemas de las RNNs: desvanecimiento y explosión del gradiente.\n    4.3.\tLSTM y GRU: soluciones al problema del desvanecimiento del gradiente.\n    4.4.\tAplicaciones de las RNNs y LSTM: procesamiento de lenguaje natural, generación de texto y análisis de series temporales.\n    4.5.\tImplementación práctica de modelos RNN y LSTM.\n:::\n\n::: {.callout-tip collapse=\"true\" icon=false}\n## Unidad 5: Introducción a Modelos Generativos\n\n    5.1.\tDiferencia entre modelos discriminativos y generativos.\n    5.2.\tConceptos básicos de Autoencoders y su aplicación en la reducción de dimensionalidad y generación de datos.\n    5.3.\tIntroducción teórica a Redes Generativas Antagónicas (GANs): fundamentos, arquitectura y aplicaciones sin implementación práctica.\n    5.4.\tDiscusión sobre el impacto de GANs en la generación de contenido y ética en la IA.\n:::\n\n# Procedimiento de evaluación\n\nA lo largo del curso, los estudiantes desarrollarán un [Proyecto aplicado]{.green} utilizando los conocimientos adquiridos durante el curso. Los estudiantes seleccionarán un problema real para aplicar técnicas de Deep Learning, incluyendo el diseño e implementación de uno o varios modelos. \n\n::: {.callout-tip appearance=\"simple\" icon=false}\n[Presentación de proyectos:]{.green}</br>\nLos estudiantes expondrán sus proyectos (máximo 3 integrantes), mostrando la metodología, resultados y aprendizajes clave. Dicho trabajo grupal tendrá 3 presentaciones de avance ([PA]{.green}) y un informe final ([IF]{.green}), con las siguientes ponderaciones:\n\n  - PA$_1$: 25$\\%$: 9 de mayo 2024\n  - PA$_2$: 25$\\%$: 13 de Junio 2024\n  - PA$_3$: 25$\\%$: 11 de Julio 2024\n  - IF: 25$\\%$: 18 de Julio 2024\n:::\n\n# Asistencia\n\n::: {.callout-tip appearance=\"simple\" icon=false}\n[Asistencia:]{.green} 75$\\%$\n:::\n\n# Bibliografía del curso\n\nLos principales libros que usaremos, además de otros recursos online son los siguientes:\n\n::: {layout-ncol=4}\n![@Goodfellow2016, [Disponible acá](https://www.deeplearningbook.org/)](images/Book_deep_learning.png){height=250px}\n\n![@Geron2022, con su repositorio gratuito en [GitHub](https://github.com/ageron/handson-ml3) ](images/Book_Hands_on_ML.jpg){height=250px}\n\n![@Raschka2019, con su repositorio gratuito en [GitHub](https://github.com/rasbt/python-machine-learning-book-3rd-edition)](images/Book_Machine_Learning.jpg){height=250px}\n\n![@Zhang2023, [Disponible acá](https://d2l.ai/)](images/Book_Dive_DL.jpg){height=250px}\n:::\n\n# Referencias\n\n::: {#refs}\n:::\n",
    "supporting": [
      "Deep_Learning_00_About_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}