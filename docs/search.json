[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco J. Plaza-Vega",
    "section": "",
    "text": "Hola!, soy un académico del Departamento de Matemática y Ciencia de la Computación, de la Universidad de Santiago de Chile.\n\n\n\n Volver arriba"
  },
  {
    "objectID": "courses/simulation/Simulation_00_About.html",
    "href": "courses/simulation/Simulation_00_About.html",
    "title": "Simulación Estadística",
    "section": "",
    "text": "1 Objetivos del curso\n\n\n\n\n\n\n\nModelar estocásticamente y simular sistemas reales simples.\nFormular modelos de simulación para sistemas reales complejos.\nVerificar y validar los modelos de simulación.\nManejar un lenguaje computacional que permita simular situaciones relativamente complejas.\n\n\n\n\n\n\n2 Programa del curso\n\n\n\n\n\n\nUnidad 0: Preliminares\n\n\n\n\n\n0.1. Probabilidades\n0.2. Probabilidad condicional e independencia\n0.3. Distribuciones de probabilidad y variables aleatorias\n0.4. Variables aleatorias\n0.5. Distribuciones de probabilidad\n0.6. Funciones de variables aleatorias\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 1: Generación de Números Pseudos Aleatorios.\n\n\n\n\n\n1.1. Aspectos históricos.\n1.2. Método Congruencial, Aditivo y Mixto.\n1.3. Método Cuadrados Medios y otros métodos para generar números pseudos aleatorios\n1.4. Test de Rachas y el test de Bondad de Ajuste para analizar la calidad de los números generados.\n1.5. Aplicación: Aproximación de integrales mediante números pseudos aleatorios.\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 2: Generación de Variables Aleatorias (discretas y continuas)\n\n\n\n\n\n2.1. Método de la transformada inversa\n2.2. Método de Aceptación y Rechazo\n2.3. Método de composición\n2.4. Método Polar para generar variables aleatorias normales\n2.5. Método Box-Muller para generar variables aleatorias normales\n2.6. Métodos para generar variables aleatorias bidimensionales: Caso Normal.\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 3: Técnicas de Reducción de Varianza.\n\n\n\n\n\n3.1. Uso de variables antitéticas.\n3.2. Uso de variables de control\n3.3. Reducción de varianza mediante condicionamiento.\n3.4. Uso de muestreo estratificado\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 4: Método de simulación por medio de eventos discretos.\n\n\n\n\n\n4.1. Generación de un Proceso de Poisson\n4.2. Generación de un Proceso de Poisson No Homogéneo.\n4.3. Simulación mediante eventos discretos.\n4.4. Sistema de líneas de espera con un servidor\n4.5. Sistema de línea de espera con dos servidores en serie.\n4.6. Sistema de línea de espera con dos servidores en paralelo.\n4.7. Aplicación de sistemas de líneas de espera mediante uso de software.\n\n\n\n\n\n3 Procedimiento de evaluación\n\n\n\n\n\n\nEvaluación: Tres pruebas escritas programadas (PEP) y trabajos grupales con ponderaciones:\n\nPEP_1: 25\\%\nPEP_2: 25\\%\nPEP_3: 25\\%\nTareas: 25\\%\n\n\n\n\n\n\n4 Asistencia\n\n\n\n\n\n\nAsistencia: 75\\%\n\n\n\n\n\n5 Bibliografía\nLos principales libros que usaremos, además de otros recursos online son los siguientes:\n\n\n\n\n\n\n\n\n\nRoss (2022)\n\n\n\n\n\n\n\nRubinstein y Kroese (2016)\n\n\n\n\n\n\n\nMorgan (2018)\n\n\n\n\n\n\n\nRizzo (2019), Con su repositorio gratuito en Github\n\n\n\n\n\n\n\n6 Referencias\n\n\nMorgan, Byron JT. 2018. Elements of simulation. Routledge.\n\n\nRizzo, Maria L. 2019. Statistical Computing with R. Second. The R Series. Chapman & Hall/CRC.\n\n\nRoss, Sheldon M. 2022. Simulation. academic press.\n\n\nRubinstein, Reuven Y, y Dirk P Kroese. 2016. Simulation and the Monte Carlo method. John Wiley & Sons.\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "courses/simulation/index.html",
    "href": "courses/simulation/index.html",
    "title": "Simulación Estadística",
    "section": "",
    "text": "1 Objetivos del curso\n\n\n\n\n\n\n\nModelar estocásticamente y simular sistemas reales simples.\nFormular modelos de simulación para sistemas reales complejos.\nVerificar y validar los modelos de simulación.\nManejar un lenguaje computacional que permita simular situaciones relativamente complejas.\n\n\n\n\n\n\n2 Programa del curso\n\n\n\n\n\n\nUnidad 0: Preliminares\n\n\n\n\n\n0.1. Probabilidades\n0.2. Probabilidad condicional e independencia\n0.3. Distribuciones de probabilidad y variables aleatorias\n0.4. Variables aleatorias\n0.5. Distribuciones de probabilidad\n0.6. Funciones de variables aleatorias\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 1: Generación de Números Pseudos Aleatorios.\n\n\n\n\n\n1.1. Aspectos históricos.\n1.2. Método Congruencial, Aditivo y Mixto.\n1.3. Método Cuadrados Medios y otros métodos para generar números pseudos aleatorios\n1.4. Test de Rachas y el test de Bondad de Ajuste para analizar la calidad de los números generados.\n1.5. Aplicación: Aproximación de integrales mediante números pseudos aleatorios.\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 2: Generación de Variables Aleatorias (discretas y continuas)\n\n\n\n\n\n2.1. Método de la transformada inversa\n2.2. Método de Aceptación y Rechazo\n2.3. Método de composición\n2.4. Método Polar para generar variables aleatorias normales\n2.5. Método Box-Muller para generar variables aleatorias normales\n2.6. Métodos para generar variables aleatorias bidimensionales: Caso Normal.\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 3: Técnicas de Reducción de Varianza.\n\n\n\n\n\n3.1. Uso de variables antitéticas.\n3.2. Uso de variables de control\n3.3. Reducción de varianza mediante condicionamiento.\n3.4. Uso de muestreo estratificado\nSlides\n\n\n\n\n\n\n\n\n\nUnidad 4: Método de simulación por medio de eventos discretos.\n\n\n\n\n\n4.1. Generación de un Proceso de Poisson\n4.2. Generación de un Proceso de Poisson No Homogéneo.\n4.3. Simulación mediante eventos discretos.\n4.4. Sistema de líneas de espera con un servidor\n4.5. Sistema de línea de espera con dos servidores en serie.\n4.6. Sistema de línea de espera con dos servidores en paralelo.\n4.7. Aplicación de sistemas de líneas de espera mediante uso de software.\n\n\n\n\n\n3 Procedimiento de evaluación\n\n\n\n\n\n\nEvaluación: Tres pruebas escritas programadas (PEP) y trabajos grupales con ponderaciones:\n\nPEP_1: 25\\%\nPEP_2: 25\\%\nPEP_3: 25\\%\nTareas: 25\\%\n\n\n\n\n\n\n4 Asistencia\n\n\n\n\n\n\nAsistencia: 75\\%\n\n\n\n\n\n5 Bibliografía\nLos principales libros que usaremos, además de otros recursos online son los siguientes:\n\n\n\n\n\n\n\n\n\nRoss (2022)\n\n\n\n\n\n\n\nRubinstein y Kroese (2016)\n\n\n\n\n\n\n\nMorgan (2018)\n\n\n\n\n\n\n\nRizzo (2019), Con su repositorio gratuito en Github\n\n\n\n\n\n\n\n6 Referencias\n\n\nMorgan, Byron JT. 2018. Elements of simulation. Routledge.\n\n\nRizzo, Maria L. 2019. Statistical Computing with R. Second. The R Series. Chapman & Hall/CRC.\n\n\nRoss, Sheldon M. 2022. Simulation. academic press.\n\n\nRubinstein, Reuven Y, y Dirk P Kroese. 2016. Simulation and the Monte Carlo method. John Wiley & Sons.\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "courses/deep_learning/Deep_Learning_00_About.html",
    "href": "courses/deep_learning/Deep_Learning_00_About.html",
    "title": "Modelos de Deep Learning",
    "section": "",
    "text": "1 Objetivos del curso\n\n\n\n\n\n\n\nComprender y aplicar los principios fundamentales de Deep Learning para modelar y resolver problemas complejos en diversos dominios.\nDiseñar, entrenar y validar modelos de Deep Learning, utilizando conjuntos de datos reales y simulados, para tareas de clasificación, regresión y otras aplicaciones avanzadas.\nEvaluar críticamente la efectividad y eficiencia de diferentes arquitecturas de redes neuronales, incluidas las redes convolucionales y recurrentes, en la solución de problemas específicos.\nDesarrollar habilidades prácticas en el uso de herramientas y librerías de software de vanguardia para el Deep Learning, tales como TensorFlow, PyTorch, y/o Keras, permitiendo la simulación y experimentación con modelos complejos.\nImplementar técnicas de optimización y ajuste fino para mejorar el rendimiento de los modelos de aprendizaje profundo, así como aplicar métodos para verificar y validar la precisión de los modelos desarrollados.\n\n\n\n\n\n\n2 Programa del curso\n\n\n\n\n\n\nUnidad 1: Fundamentos del Deep Learning\n\n\n\n\n\n1.1. Introducción al Deep Learning y su evolución histórica.\n1.2. Principios básicos de las redes neuronales: perceptrones, perceptrones multicapa (MLP), funciones de activación, y propagación hacia adelante.\n1.3. Entrenamiento de redes neuronales profundas: backpropagation, descenso del gradiente y ajuste de hiperparámetros. Uso de la librería Keras para facilitar la implementación de modelos.\n1.4. Evaluación de modelos: conjuntos de entrenamiento, validación y prueba; sobreajuste y técnicas de regularización.\n1.5. Introducción a TensorFlow, PyTorch y Keras como interfaz de alto nivel para la construcción y el entrenamiento de modelos de deep learning.\nSlides  Tutorial básico de Python\n\n\n\n\n\n\n\n\n\nUnidad 2: Perceptrones Multicapa (MLP) y Deep Learning\n\n\n\n\n\n2.1. Estructura y características de los MLP: capas ocultas, nodos y profundidad.\n2.2. Aplicaciones prácticas de los MLP en clasificación y regresión.\n2.3. Comparación entre MLP y otros modelos de Deep Learning en términos de capacidad, complejidad y tipos de problemas a resolver.\n2.4. Implementación de MLP para tareas de clasificación y regresión.\n2.5. Estrategias de optimización y ajuste fino específicas para MLP.\nSlides  Guía de Keras  Ejemplo 1: Librería neuralnet en R  Ejemplo 2: Librería keras con datos de MNIST Ejemplo 3: Librería keras para regresión con datos de automóviles\n\n\n\n\n\n\n\n\n\nUnidad 3: Redes Neuronales Convolucionales (CNNs)\n\n\n\n\n\n3.1.    Fundamentos de las CNNs: operación de convolución, pooling y normalización.\n3.2.    Arquitecturas de CNNs populares: LeNet, AlexNet, VGG, ResNet e Inception.\n3.3.    Introducción al Transfer Learning en CNNs: cómo reutilizar modelos preentrenados para nuevas tareas.\n3.4.    Aplicaciones de las CNNs: reconocimiento de imágenes, detección de objetos y segmentación semántica.\n3.5.    Prácticas con conjuntos de datos reales utilizando librerías de Deep Learning.\nSlides  Ejemplo 1: Librería Keras con datos fashion-MNIST  Ejemplo 2: Gatos y perros  Ejemplo 2b: Gatos y perros, la venganza\n\n\n\n\n\n\n\n\n\nUnidad 4: Redes Neuronales Recurrentes (RNNs) y LSTM\n\n\n\n\n\n4.1.    Principios de las RNNs: procesamiento de secuencias y dependencias temporales.\n4.2.    Problemas de las RNNs: desvanecimiento y explosión del gradiente.\n4.3.    LSTM y GRU: soluciones al problema del desvanecimiento del gradiente.\n4.4.    Aplicaciones de las RNNs y LSTM: procesamiento de lenguaje natural, generación de texto y análisis de series temporales.\n4.5.    Implementación práctica de modelos RNN y LSTM.\n\n\n\n\n\n\n\n\n\nUnidad 5: Introducción a Modelos Generativos\n\n\n\n\n\n5.1.    Diferencia entre modelos discriminativos y generativos.\n5.2.    Conceptos básicos de Autoencoders y su aplicación en la reducción de dimensionalidad y generación de datos.\n5.3.    Introducción teórica a Redes Generativas Antagónicas (GANs): fundamentos, arquitectura y aplicaciones sin implementación práctica.\n5.4.    Discusión sobre el impacto de GANs en la generación de contenido y ética en la IA.\n\n\n\n\n\n3 Procedimiento de evaluación\nA lo largo del curso, los estudiantes desarrollarán un Proyecto aplicado utilizando los conocimientos adquiridos durante el curso. Los estudiantes seleccionarán un problema real para aplicar técnicas de Deep Learning, incluyendo el diseño e implementación de uno o varios modelos.\n\n\n\n\n\n\nPresentación de proyectos: Los estudiantes expondrán sus proyectos (máximo 3 integrantes), mostrando la metodología, resultados y aprendizajes clave. Dicho trabajo grupal tendrá 3 presentaciones de avance (PA) y un informe final (IF), con las siguientes ponderaciones:\n\nPA_1: 25\\%: 9 de mayo 2024\nPA_2: 25\\%: 13 de Junio 2024\nPA_3: 25\\%: 11 de Julio 2024\nIF: 25\\%: 18 de Julio 2024\n\n\n\n\n\n\n4 Asistencia\n\n\n\n\n\n\nAsistencia: 75\\%\n\n\n\n\n\n5 Bibliografía del curso\nLos principales libros que usaremos, además de otros recursos online son los siguientes:\n\n\n\n\n\n\n\n\n\nGoodfellow, Bengio, y Courville (2016), Disponible acá\n\n\n\n\n\n\n\nGéron (2022), con su repositorio gratuito en GitHub\n\n\n\n\n\n\n\nRaschka y Mirjalili (2019), con su repositorio gratuito en GitHub\n\n\n\n\n\n\n\nZhang et al. (2023), Disponible acá\n\n\n\n\n\n\n\n6 Referencias\n\n\nGéron, Aurélien. 2022. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep learning. MIT press.\n\n\nRaschka, Sebastian, y Vahid Mirjalili. 2019. Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing Ltd.\n\n\nZhang, Aston, Zachary C Lipton, Mu Li, y Alexander J Smola. 2023. Dive into deep learning. Cambridge University Press.\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "courses/deep_learning/index.html",
    "href": "courses/deep_learning/index.html",
    "title": "Modelos de Deep Learning",
    "section": "",
    "text": "1 Objetivos del curso\n\n\n\n\n\n\n\nComprender y aplicar los principios fundamentales de Deep Learning para modelar y resolver problemas complejos en diversos dominios.\nDiseñar, entrenar y validar modelos de Deep Learning, utilizando conjuntos de datos reales y simulados, para tareas de clasificación, regresión y otras aplicaciones avanzadas.\nEvaluar críticamente la efectividad y eficiencia de diferentes arquitecturas de redes neuronales, incluidas las redes convolucionales y recurrentes, en la solución de problemas específicos.\nDesarrollar habilidades prácticas en el uso de herramientas y librerías de software de vanguardia para el Deep Learning, tales como TensorFlow, PyTorch, y/o Keras, permitiendo la simulación y experimentación con modelos complejos.\nImplementar técnicas de optimización y ajuste fino para mejorar el rendimiento de los modelos de aprendizaje profundo, así como aplicar métodos para verificar y validar la precisión de los modelos desarrollados.\n\n\n\n\n\n\n2 Programa del curso\n\n\n\n\n\n\nUnidad 1: Fundamentos del Deep Learning\n\n\n\n\n\n1.1. Introducción al Deep Learning y su evolución histórica.\n1.2. Principios básicos de las redes neuronales: perceptrones, perceptrones multicapa (MLP), funciones de activación, y propagación hacia adelante.\n1.3. Entrenamiento de redes neuronales profundas: backpropagation, descenso del gradiente y ajuste de hiperparámetros. Uso de la librería Keras para facilitar la implementación de modelos.\n1.4. Evaluación de modelos: conjuntos de entrenamiento, validación y prueba; sobreajuste y técnicas de regularización.\n1.5. Introducción a TensorFlow, PyTorch y Keras como interfaz de alto nivel para la construcción y el entrenamiento de modelos de deep learning.\nSlides  Tutorial básico de Python\n\n\n\n\n\n\n\n\n\nUnidad 2: Perceptrones Multicapa (MLP) y Deep Learning\n\n\n\n\n\n2.1. Estructura y características de los MLP: capas ocultas, nodos y profundidad.\n2.2. Aplicaciones prácticas de los MLP en clasificación y regresión.\n2.3. Comparación entre MLP y otros modelos de Deep Learning en términos de capacidad, complejidad y tipos de problemas a resolver.\n2.4. Implementación de MLP para tareas de clasificación y regresión.\n2.5. Estrategias de optimización y ajuste fino específicas para MLP.\nSlides  Guía de Keras  Ejemplo 1: Librería neuralnet en R  Ejemplo 2: Librería keras con datos de MNIST Ejemplo 3: Librería keras para regresión con datos de automóviles\n\n\n\n\n\n\n\n\n\nUnidad 3: Redes Neuronales Convolucionales (CNNs)\n\n\n\n\n\n3.1.    Fundamentos de las CNNs: operación de convolución, pooling y normalización.\n3.2.    Arquitecturas de CNNs populares: LeNet, AlexNet, VGG, ResNet e Inception.\n3.3.    Introducción al Transfer Learning en CNNs: cómo reutilizar modelos preentrenados para nuevas tareas.\n3.4.    Aplicaciones de las CNNs: reconocimiento de imágenes, detección de objetos y segmentación semántica.\n3.5.    Prácticas con conjuntos de datos reales utilizando librerías de Deep Learning.\nSlides  Ejemplo 1: Librería Keras con datos fashion-MNIST  Ejemplo 2: Gatos y perros  Ejemplo 2b: Gatos y perros, la venganza\n\n\n\n\n\n\n\n\n\nUnidad 4: Redes Neuronales Recurrentes (RNNs) y LSTM\n\n\n\n\n\n4.1.    Principios de las RNNs: procesamiento de secuencias y dependencias temporales.\n4.2.    Problemas de las RNNs: desvanecimiento y explosión del gradiente.\n4.3.    LSTM y GRU: soluciones al problema del desvanecimiento del gradiente.\n4.4.    Aplicaciones de las RNNs y LSTM: procesamiento de lenguaje natural, generación de texto y análisis de series temporales.\n4.5.    Implementación práctica de modelos RNN y LSTM.\n\n\n\n\n\n\n\n\n\nUnidad 5: Introducción a Modelos Generativos\n\n\n\n\n\n5.1.    Diferencia entre modelos discriminativos y generativos.\n5.2.    Conceptos básicos de Autoencoders y su aplicación en la reducción de dimensionalidad y generación de datos.\n5.3.    Introducción teórica a Redes Generativas Antagónicas (GANs): fundamentos, arquitectura y aplicaciones sin implementación práctica.\n5.4.    Discusión sobre el impacto de GANs en la generación de contenido y ética en la IA.\n\n\n\n\n\n3 Procedimiento de evaluación\nA lo largo del curso, los estudiantes desarrollarán un Proyecto aplicado utilizando los conocimientos adquiridos durante el curso. Los estudiantes seleccionarán un problema real para aplicar técnicas de Deep Learning, incluyendo el diseño e implementación de uno o varios modelos.\n\n\n\n\n\n\nPresentación de proyectos: Los estudiantes expondrán sus proyectos (máximo 3 integrantes), mostrando la metodología, resultados y aprendizajes clave. Dicho trabajo grupal tendrá 3 presentaciones de avance (PA) y un informe final (IF), con las siguientes ponderaciones:\n\nPA_1: 25\\%\nPA_2: 25\\%\nPA_3: 25\\%\nIF: 25\\%\n\n\n\n\n\n\n4 Asistencia\n\n\n\n\n\n\nAsistencia: 75\\%\n\n\n\n\n\n5 Bibliografía del curso\nLos principales libros que usaremos, además de otros recursos online son los siguientes:\n\n\n\n\n\n\n\n\n\nGoodfellow, Bengio, y Courville (2016), Disponible acá\n\n\n\n\n\n\n\nGéron (2022), con su repositorio gratuito en GitHub\n\n\n\n\n\n\n\nRaschka y Mirjalili (2019), con su repositorio gratuito en GitHub\n\n\n\n\n\n\n\nZhang et al. (2023), Disponible acá\n\n\n\n\n\n\n\n6 Referencias\n\n\nGéron, Aurélien. 2022. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media, Inc.\n\n\nGoodfellow, Ian, Yoshua Bengio, y Aaron Courville. 2016. Deep learning. MIT press.\n\n\nRaschka, Sebastian, y Vahid Mirjalili. 2019. Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing Ltd.\n\n\nZhang, Aston, Zachary C Lipton, Mu Li, y Alexander J Smola. 2023. Dive into deep learning. Cambridge University Press.\n\n\n\n\n\n\n Volver arriba"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Publicaciones",
    "section": "",
    "text": "Anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax) variability changes in northern Chile associated with the environment and inter species synchronicity: GARCH model with exogenous variable and hybrid Bayesian deep learning estimation approach\n\n\nblablablab blablablabla.\n\n\n\nResearch\n\n\nGARCH-X\n\n\nTime-Series\n\n\n\n\n\n\n4 ene 2024\n\n\nFrancisco Plaza-Vega, Héctor Araya\n\n\n\n\n\n\nNo hay resultados\n\n Volver arriba"
  },
  {
    "objectID": "research/2024-09-22-test/index.html",
    "href": "research/2024-09-22-test/index.html",
    "title": "Anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax) variability changes in northern Chile associated with the environment and inter species synchronicity: GARCH model with exogenous variable and hybrid Bayesian deep learning estimation approach",
    "section": "",
    "text": "Small pelagic fish species, such as anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax), play a crucial role in marine ecosystems worldwide as they serve as an important food source for higher-order predators, such as seabirds, marine mammals, and larger fish species; also from their high productivity in terms of fishery landings, they help with maintaining food security. However, small pelagic populations are known for their variability, with fluctuations in their distribution and abundance influenced by interactions with environmental and human-related factors in different spatio-temporal scales. This study aims to investigate the variability in anchovy and sardine populations in northern Chile and their potential environmental drivers by using a class of models that address the changes in variability (i.e. variance) over time, namely Generalized autoregressive conditional heteroskedasticity (GARCH) models. In particular, this work considers a GARCH model that includes an additional term in the variance, that could help to better model the variability fluctuations of both anchovy and sardine, explained by environmental factors. Since there is no estimation procedures for those type of models, we propose a hybrid Approximate Bayesian Computation (ABC) procedure that involves the use of Deep Learning structures for estimating the parameters of the model and obtain posterior distributions. The results are two-fold: First, the proposal of a new time series model that better explain conditional variance with exogenous variables and a novel estimation procedure, and second, a novel approach for establishing explicit models that address the variability of small pelagic fisheries and their interaction with the environment."
  },
  {
    "objectID": "research/2024-09-22-test/index.html#abstract",
    "href": "research/2024-09-22-test/index.html#abstract",
    "title": "Anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax) variability changes in northern Chile associated with the environment and inter species synchronicity: GARCH model with exogenous variable and hybrid Bayesian deep learning estimation approach",
    "section": "",
    "text": "Small pelagic fish species, such as anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax), play a crucial role in marine ecosystems worldwide as they serve as an important food source for higher-order predators, such as seabirds, marine mammals, and larger fish species; also from their high productivity in terms of fishery landings, they help with maintaining food security. However, small pelagic populations are known for their variability, with fluctuations in their distribution and abundance influenced by interactions with environmental and human-related factors in different spatio-temporal scales. This study aims to investigate the variability in anchovy and sardine populations in northern Chile and their potential environmental drivers by using a class of models that address the changes in variability (i.e. variance) over time, namely Generalized autoregressive conditional heteroskedasticity (GARCH) models. In particular, this work considers a GARCH model that includes an additional term in the variance, that could help to better model the variability fluctuations of both anchovy and sardine, explained by environmental factors. Since there is no estimation procedures for those type of models, we propose a hybrid Approximate Bayesian Computation (ABC) procedure that involves the use of Deep Learning structures for estimating the parameters of the model and obtain posterior distributions. The results are two-fold: First, the proposal of a new time series model that better explain conditional variance with exogenous variables and a novel estimation procedure, and second, a novel approach for establishing explicit models that address the variability of small pelagic fisheries and their interaction with the environment."
  },
  {
    "objectID": "research/2024-01-04-garchx-anchovy-sardine/index.html",
    "href": "research/2024-01-04-garchx-anchovy-sardine/index.html",
    "title": "Anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax) variability changes in northern Chile associated with the environment and inter species synchronicity: GARCH model with exogenous variable and hybrid Bayesian deep learning estimation approach",
    "section": "",
    "text": "Small pelagic fish species, such as anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax), play a crucial role in marine ecosystems worldwide as they serve as an important food source for higher-order predators, such as seabirds, marine mammals, and larger fish species; also from their high productivity in terms of fishery landings, they help with maintaining food security. However, small pelagic populations are known for their variability, with fluctuations in their distribution and abundance influenced by interactions with environmental and human-related factors in different spatio-temporal scales. This study aims to investigate the variability in anchovy and sardine populations in northern Chile and their potential environmental drivers by using a class of models that address the changes in variability (i.e. variance) over time, namely Generalized autoregressive conditional heteroskedasticity (GARCH) models. In particular, this work considers a GARCH model that includes an additional term in the variance, that could help to better model the variability fluctuations of both anchovy and sardine, explained by environmental factors. Since there is no estimation procedures for those type of models, we propose a hybrid Approximate Bayesian Computation (ABC) procedure that involves the use of Deep Learning structures for estimating the parameters of the model and obtain posterior distributions. The results are two-fold: First, the proposal of a new time series model that better explain conditional variance with exogenous variables and a novel estimation procedure, and second, a novel approach for establishing explicit models that address the variability of small pelagic fisheries and their interaction with the environment."
  },
  {
    "objectID": "research/2024-01-04-garchx-anchovy-sardine/index.html#abstract",
    "href": "research/2024-01-04-garchx-anchovy-sardine/index.html#abstract",
    "title": "Anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax) variability changes in northern Chile associated with the environment and inter species synchronicity: GARCH model with exogenous variable and hybrid Bayesian deep learning estimation approach",
    "section": "",
    "text": "Small pelagic fish species, such as anchovy (Engraulis ringens) and Pacific sardine (Sardinops sagax), play a crucial role in marine ecosystems worldwide as they serve as an important food source for higher-order predators, such as seabirds, marine mammals, and larger fish species; also from their high productivity in terms of fishery landings, they help with maintaining food security. However, small pelagic populations are known for their variability, with fluctuations in their distribution and abundance influenced by interactions with environmental and human-related factors in different spatio-temporal scales. This study aims to investigate the variability in anchovy and sardine populations in northern Chile and their potential environmental drivers by using a class of models that address the changes in variability (i.e. variance) over time, namely Generalized autoregressive conditional heteroskedasticity (GARCH) models. In particular, this work considers a GARCH model that includes an additional term in the variance, that could help to better model the variability fluctuations of both anchovy and sardine, explained by environmental factors. Since there is no estimation procedures for those type of models, we propose a hybrid Approximate Bayesian Computation (ABC) procedure that involves the use of Deep Learning structures for estimating the parameters of the model and obtain posterior distributions. The results are two-fold: First, the proposal of a new time series model that better explain conditional variance with exogenous variables and a novel estimation procedure, and second, a novel approach for establishing explicit models that address the variability of small pelagic fisheries and their interaction with the environment."
  }
]